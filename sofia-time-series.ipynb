{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bme_file_location = \"sofia_small/*bme280sof.csv\"\n",
    "sds_file_location = \"sofia_small/*sds011sof.csv\"\n",
    "\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "df_bme = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .option(\"header\", first_row_is_header) \\\n",
    "    .option(\"sep\", delimiter) \\\n",
    "    .load(bme_file_location)\n",
    "\n",
    "df_sds = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .option(\"header\", first_row_is_header) \\\n",
    "    .option(\"sep\", delimiter) \\\n",
    "    .load(sds_file_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|        ts|           avg(P1)|           avg(P2)|\n",
      "+----------+------------------+------------------+\n",
      "|2017-07-01|17.764459663706905| 8.341274009698298|\n",
      "|2017-07-02| 9.846284524930946| 6.325375406399083|\n",
      "|2017-07-03| 20.35557791635185|17.195223293020778|\n",
      "|2017-07-04| 8.984114511906204| 6.868896334621589|\n",
      "|2017-07-05|10.412705222705204| 7.964031059031034|\n",
      "|2017-07-06| 10.85810864999049| 8.447780535930221|\n",
      "|2017-07-07| 9.614079073024804| 7.430200547526521|\n",
      "|2017-07-08| 12.10184730986929| 9.885236809576535|\n",
      "|2017-07-09|12.441132935466957|10.319859653725107|\n",
      "|2017-07-10|14.278580865387667|12.425794746989531|\n",
      "|2017-07-11|16.458481004748865|13.907630592351836|\n",
      "|2017-07-12|14.077904752827688|10.800456856017346|\n",
      "|2017-07-13| 11.50965046888325| 8.878007956805918|\n",
      "|2017-07-14| 5.461827450735781|  3.10989585931652|\n",
      "|2017-07-15|10.245437171815821| 7.799760959824183|\n",
      "|2017-07-16|11.484685678666041|  9.46174505220515|\n",
      "|2017-07-17| 8.730244358596998|  7.05922492028453|\n",
      "|2017-07-18|11.758467153284702| 9.619662928016169|\n",
      "|2017-07-19|10.757269963337261| 8.447159080747532|\n",
      "|2017-07-20|10.377995553018224| 8.262260148432995|\n",
      "+----------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import to_timestamp,date_format\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import count, avg\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "df_sds_transformed = df_sds.withColumn('year',year(df_sds.timestamp))\\\n",
    "    .withColumn('month', month(df_sds.timestamp))\\\n",
    "    .withColumn(\"day\", date_format(col(\"timestamp\"), \"d\"))\\\n",
    "    .withColumn(\"ts\", to_date(col(\"timestamp\")).cast(\"date\"))\n",
    "\n",
    "df_sds_transformed = df_sds_transformed.groupBy(\"ts\").agg(avg(\"P1\"), avg(\"P2\")).orderBy([\"ts\"], ascending=True)\n",
    "\n",
    "df_sds_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+------------------+\n",
      "|        ts|    avg(pressure)|  avg(temperature)|     avg(humidity)|\n",
      "+----------+-----------------+------------------+------------------+\n",
      "|2017-07-01|94572.18985080464| 33.33327613327619|32.792403355736745|\n",
      "|2017-07-02|94441.42854684066|28.197254514672572| 44.52180304740427|\n",
      "|2017-07-03|94668.76243252479| 18.25461707200767| 78.17694325226547|\n",
      "|2017-07-04|95313.96683276288| 22.32803235375923|  50.4074079911003|\n",
      "|2017-07-05|95440.82530922632|23.534423652694652|44.841247660928104|\n",
      "|2017-07-06|95312.02019876736|25.778363851992424| 42.49701185958226|\n",
      "|2017-07-07|95248.96706425186|27.469182004089852|40.482749797878675|\n",
      "|2017-07-08|95059.96317789162|  25.7144688644688| 51.47889969005336|\n",
      "|2017-07-09|95089.78527820377|27.075451422027033| 49.46747614048477|\n",
      "|2017-07-10| 95128.1010232264|28.758966410703227|44.910974230932034|\n",
      "|2017-07-11|95059.89666140139|30.580405242122854| 41.59478715493988|\n",
      "|2017-07-12|94791.26359009359| 30.41663144117502| 36.67443695768059|\n",
      "|2017-07-13| 94660.5128211271|28.239342143762816|  40.2070233102511|\n",
      "|2017-07-14|94698.49576498086|25.985152705061278| 35.94933100639909|\n",
      "|2017-07-15|94722.05143758388|23.879997175673175| 48.98377471286015|\n",
      "|2017-07-16|95101.30591753831|17.941519693602615| 71.34576150172559|\n",
      "|2017-07-17|95316.44255811958|18.957777725805787| 65.99736470368151|\n",
      "|2017-07-18|95171.17375061983|22.368150564730172| 55.49259010934609|\n",
      "|2017-07-19|94549.30462410931|24.076660417047684| 47.53020578013586|\n",
      "|2017-07-20|94961.17755593521| 25.72600174937875| 43.76265629315902|\n",
      "+----------+-----------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bme_transformed = df_bme.withColumn('year',year(df_bme.timestamp))\\\n",
    "    .withColumn('month', month(df_bme.timestamp))\\\n",
    "    .withColumn(\"day\", date_format(col(\"timestamp\"), \"d\"))\\\n",
    "    .withColumn(\"ts\", to_date(col(\"timestamp\")).cast(\"date\"))\n",
    "\n",
    "df_bme_transformed = df_bme_transformed.groupBy(\"ts\").agg(avg(\"pressure\"), avg(\"temperature\"), avg(\"humidity\"))\\\n",
    "    .orderBy([\"ts\"], ascending=True)\n",
    "\n",
    "df_bme_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|        ts|    avg(pressure)|  avg(temperature)|     avg(humidity)|           avg(P1)|           avg(P2)|\n",
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|2017-07-01|94572.18985080464| 33.33327613327619|32.792403355736745|17.764459663706905| 8.341274009698298|\n",
      "|2017-07-02|94441.42854684066|28.197254514672572| 44.52180304740427| 9.846284524930946| 6.325375406399083|\n",
      "|2017-07-03|94668.76243252479| 18.25461707200767| 78.17694325226547| 20.35557791635185|17.195223293020778|\n",
      "|2017-07-04|95313.96683276288| 22.32803235375923|  50.4074079911003| 8.984114511906204| 6.868896334621589|\n",
      "|2017-07-05|95440.82530922632|23.534423652694652|44.841247660928104|10.412705222705204| 7.964031059031034|\n",
      "|2017-07-06|95312.02019876736|25.778363851992424| 42.49701185958226| 10.85810864999049| 8.447780535930221|\n",
      "|2017-07-07|95248.96706425186|27.469182004089852|40.482749797878675| 9.614079073024804| 7.430200547526521|\n",
      "|2017-07-08|95059.96317789162|  25.7144688644688| 51.47889969005336| 12.10184730986929| 9.885236809576535|\n",
      "|2017-07-09|95089.78527820377|27.075451422027033| 49.46747614048477|12.441132935466957|10.319859653725107|\n",
      "|2017-07-10| 95128.1010232264|28.758966410703227|44.910974230932034|14.278580865387667|12.425794746989531|\n",
      "|2017-07-11|95059.89666140139|30.580405242122854| 41.59478715493988|16.458481004748865|13.907630592351836|\n",
      "|2017-07-12|94791.26359009359| 30.41663144117502| 36.67443695768059|14.077904752827688|10.800456856017346|\n",
      "|2017-07-13| 94660.5128211271|28.239342143762816|  40.2070233102511| 11.50965046888325| 8.878007956805918|\n",
      "|2017-07-14|94698.49576498086|25.985152705061278| 35.94933100639909| 5.461827450735781|  3.10989585931652|\n",
      "|2017-07-15|94722.05143758388|23.879997175673175| 48.98377471286015|10.245437171815821| 7.799760959824183|\n",
      "|2017-07-16|95101.30591753831|17.941519693602615| 71.34576150172559|11.484685678666041|  9.46174505220515|\n",
      "|2017-07-17|95316.44255811958|18.957777725805787| 65.99736470368151| 8.730244358596998|  7.05922492028453|\n",
      "|2017-07-18|95171.17375061983|22.368150564730172| 55.49259010934609|11.758467153284702| 9.619662928016169|\n",
      "|2017-07-19|94549.30462410931|24.076660417047684| 47.53020578013586|10.757269963337261| 8.447159080747532|\n",
      "|2017-07-20|94961.17755593521| 25.72600174937875| 43.76265629315902|10.377995553018224| 8.262260148432995|\n",
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df = df_bme_transformed.join(df_sds_transformed, on=['ts'], how='left').orderBy([\"ts\"], ascending=True)\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 9\n",
      "objectiveHistory: [0.5, 0.46857330098728367, 0.3658516091389527, 0.36394065583390095, 0.35833477412709724, 0.3577458483095399, 0.35763854914040893, 0.3576385069330923, 0.357638506892395]\n",
      "RMSE: 25.840254\n",
      "r2: 0.294767\n",
      "+-------+------------------+\n",
      "|summary|           avg(P1)|\n",
      "+-------+------------------+\n",
      "|  count|               184|\n",
      "|   mean| 29.66323086015132|\n",
      "| stddev|30.854169933459772|\n",
      "|    min| 5.461827450735781|\n",
      "|    max|160.07202801241624|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['avg(pressure)', 'avg(temperature)', 'avg(humidity)'], outputCol = 'features')\n",
    "features_df = vectorAssembler.transform(combined_df)\n",
    "features_df = features_df.select(['features', 'avg(P1)'])\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='avg(P1)', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(features_df)\n",
    "\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "#trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "features_df.describe('avg(P1)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Function Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|        ts|    avg(pressure)|  avg(temperature)|     avg(humidity)|           avg(P1)|      prev_avg(P1)|\n",
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|2017-07-01|94572.18985080464| 33.33327613327619|32.792403355736745|17.764459663706905|              null|\n",
      "|2017-07-02|94441.42854684066|28.197254514672572| 44.52180304740427| 9.846284524930946|17.764459663706905|\n",
      "|2017-07-03|94668.76243252479| 18.25461707200767| 78.17694325226547| 20.35557791635185| 9.846284524930946|\n",
      "|2017-07-04|95313.96683276288| 22.32803235375923|  50.4074079911003| 8.984114511906204| 20.35557791635185|\n",
      "|2017-07-05|95440.82530922632|23.534423652694652|44.841247660928104|10.412705222705204| 8.984114511906204|\n",
      "|2017-07-06|95312.02019876736|25.778363851992424| 42.49701185958226| 10.85810864999049|10.412705222705204|\n",
      "|2017-07-07|95248.96706425186|27.469182004089852|40.482749797878675| 9.614079073024804| 10.85810864999049|\n",
      "|2017-07-08|95059.96317789162|  25.7144688644688| 51.47889969005336| 12.10184730986929| 9.614079073024804|\n",
      "|2017-07-09|95089.78527820377|27.075451422027033| 49.46747614048477|12.441132935466957| 12.10184730986929|\n",
      "|2017-07-10| 95128.1010232264|28.758966410703227|44.910974230932034|14.278580865387667|12.441132935466957|\n",
      "|2017-07-11|95059.89666140139|30.580405242122854| 41.59478715493988|16.458481004748865|14.278580865387667|\n",
      "|2017-07-12|94791.26359009359| 30.41663144117502| 36.67443695768059|14.077904752827688|16.458481004748865|\n",
      "|2017-07-13| 94660.5128211271|28.239342143762816|  40.2070233102511| 11.50965046888325|14.077904752827688|\n",
      "|2017-07-14|94698.49576498086|25.985152705061278| 35.94933100639909| 5.461827450735781| 11.50965046888325|\n",
      "|2017-07-15|94722.05143758388|23.879997175673175| 48.98377471286015|10.245437171815821| 5.461827450735781|\n",
      "|2017-07-16|95101.30591753831|17.941519693602615| 71.34576150172559|11.484685678666041|10.245437171815821|\n",
      "|2017-07-17|95316.44255811958|18.957777725805787| 65.99736470368151| 8.730244358596998|11.484685678666041|\n",
      "|2017-07-18|95171.17375061983|22.368150564730172| 55.49259010934609|11.758467153284702| 8.730244358596998|\n",
      "|2017-07-19|94549.30462410931|24.076660417047684| 47.53020578013586|10.757269963337261|11.758467153284702|\n",
      "|2017-07-20|94961.17755593521| 25.72600174937875| 43.76265629315902|10.377995553018224|10.757269963337261|\n",
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "w = Window.orderBy(\"ts\")\n",
    "df1 = combined_df.withColumn(\"prev_avg(P1)\", F.lag(\"avg(P1)\").over(w))\n",
    "df1.drop(\"avg(P2)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lags = combined_df.select(\"ts\", \"avg(P1)\")\n",
    "\n",
    "for i in range(1, 8): \n",
    "    df_lags = df_lags.withColumn(f\"P1_lag_{i}\", F.lag(F.col('avg(P1)'), i).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_feature_df = df_lags.select(\"P1_lag_1\", \"P1_lag_2\", \"P1_lag_3\", \"P1_lag_4\", \"P1_lag_5\", \"P1_lag_6\", \"P1_lag_7\", \"avg(P1)\")\n",
    "lag_feature_df = lag_feature_df.where(col(\"P1_lag_7\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagging P1 values from the past 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 22.740847\n",
      "r2: 0.467686\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = [\"P1_lag_1\", \"P1_lag_2\", \"P1_lag_3\", \"P1_lag_4\", \"P1_lag_5\", \"P1_lag_6\", \"P1_lag_7\"], outputCol = 'features')\n",
    "lag_df = vectorAssembler.transform(lag_feature_df)\n",
    "lag_df = lag_df.select(['features', 'avg(P1)'])\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='avg(P1)', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(lag_df)\n",
    "\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagging P1 values from past  7 days AND lagging temps / pressures / humidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert them into long rows with the lag information next...\n",
    "         # groupBy sensor?\n",
    "         # get averages for each day for each sensor\n",
    "         # take the past 7 days lag information for variables\n",
    "         # https://www.slideshare.net/SparkSummit/time-series-analytics-with-spark-spark-summit-east-talk-by-simon-ouellette\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@sergey.ivanchuk/practical-pyspark-window-function-examples-cb5c7e1a3c41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
